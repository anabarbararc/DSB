{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc95005",
   "metadata": {},
   "source": [
    "# Chapter 5\n",
    "____________________\n",
    "# Overfitting and its avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519c958",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model detects interesting patterns in the data but does not generalize well.\n",
    "\n",
    "When reading the churn example given in the book, it reminded me of studying in university for a exam. Imagine you have access to previous Physics I exams given by your professor. I would say there are two ways you could study for this exam: \n",
    " - 1) you could memorize all questions and solutions\n",
    " - 2) create a methodical procedure to solve any problem\n",
    "\n",
    "The first method would give you the highest score considering your professor only uses previous questions in the exam you actually have to do,  but it won't work well if there are new questions.\n",
    "\n",
    "The second method generalises any Physics I problem.\n",
    "\n",
    "Overfitting is the tendency to tailor models to the training data.\n",
    "\n",
    "Evaluation on the training data don't tell us how good the model perform on unseen data. To estimate it, we use a hold-out set (or test set) in which we don't use our model to be trained.\n",
    "\n",
    "## Overfitting : Trees\n",
    "\n",
    "Growing a tree until it has only pure leaves tends to overfit. It is not like the <i>memorization</i> problem, because it will arrive at some nontrivial classification even for instances it hasn't seen before.\n",
    "\n",
    "In the begining of the training, the tree is very small and has poor performance. As the number of nodes grows, the accuracy of the model increses in both training and hold-out sets, but not forever. There is a <i>sweet spot</i> where the model starts picking up more patterns in the training set and overfits, improving the accuracy of the model in the training set, but the accuracy on the hold-out set is going to start decreasing.\n",
    "\n",
    "## Overfitting : Mathematical Functions\n",
    "\n",
    "Too many attributes to describe a dataset lead to overfitting. To deal with this issue, we use a technique called feature selection to assess information of the accuracy or score of the model when adding the attribute to the model.\n",
    "\n",
    "### Example\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src='ex1.png' width='300'/> </td> \n",
    "    <td> <img src='ex2.png' width='300'/> </td>   \n",
    "    <td> <img src='ex3.png' width='300'/> </td>   \n",
    "</tr></table>\n",
    "\n",
    "In the image below, we can observe two models (Logistic Regression and SVM) classifying data from the Iris dataset. In the middle and right plot, an outlier was added. It is clear the Logistic Regression model tries to accomodate for the outlier while the SVM model is stable. The SVM tends to be less sensitive to individual examples and its training procedure incorporates complexity control.\n",
    "\n",
    "<img src='ex4.png' width='300'/>\n",
    "\n",
    "In the picture above, another feature was added: `sepal width^2`. This allows adds more complexity to the model leading to non-linear boundary curves. It also gives the methods far more opportunity to overfit.  The SVM model, even though its boundary now is curved, the training procedure still has opted for the larger margin around the boundary, rather than the perfect separation of the positive different classes.\n",
    "\n",
    "## * Example: Why Is Overfitting Bad?\n",
    "\n",
    "When a model gets more complex it is allowed to pick up harmful spurious correlations specific of the training dataset, generating incorrect generalizations in the model.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "Cross-validation is a more sophisticated holdout training and testing procedure. It allows to estimate the generalization performance across the dataset, its mean and variance. \n",
    "\n",
    "Cross-validation computes its estimates over all the data by performing multiple splits and systematically swapping out samples for testing. It begins by splitting a labeled dataset into $k$ partitions called <i>folds</i>. Cross-validation then iterates training and testing k times, in a particular way. In each iteration of the cross-validation, a different fold is chosen as the test data; we have $(k– 1)/k$ of the data used for training and $1/k$ used for testing. Each iteration produces one model, and thereby one estimate of generalization performance. When cross-validation is finished, every example will have been used only once for testing but $k–1$ times for training.\n",
    "\n",
    "## Learning Curves\n",
    "\n",
    "The generalization performance of a model improves the more data is available for training, but it eventlually flattens out.\n",
    "\n",
    "<i>Learning curve</i> is the plot of the generalization performance of the model vs the amount of training data.\n",
    "\n",
    "Usually it has a characteristic shape. Initially, the performace is low. The curve is very steep as the model gains more information about the data patterns as the the training data increases, this the model gets more accurate. Eventually the curve becomes less steep, as the gain of information does not give more model accuracy; the curve becomes more flatten.\n",
    "\n",
    "An example is given considering learning curves from two models: Decition Tree and Logistic Regression analysing the same data. \n",
    "\n",
    "<img src='ex5.png' width='300'/>\n",
    "\n",
    "There is some interesting conclusion about these two models:\n",
    "- Given the same set of features, Decision Tree is more flexible than Logistic Regression\n",
    "- For larger datasets, Decision Trees will tend to overfit more.\n",
    "- In general, Logistic Regression will perform better on smaller datasets.\n",
    "\n",
    "What is important about the learning curve is that it can help in decision taking. For example: More data will help with model perfomance? If the learcing curve is flatten out, the answer is no, and, to improve the model, we might need to tweek its hyperparameters.\n",
    "\n",
    "## Overfitting Avoidance and Complexity Control\n",
    "\n",
    "### Avoiding Overfitting with Tree Induction\n",
    "It is the easiest to overfit.\n",
    "\n",
    "Tree induction (I call them Decision Trees) have typically two methods to avoid overfitting:\n",
    "   1. stop growing the tree before it gets too complex\n",
    "   \n",
    "   <p>Researches develpoed a method based on hyphothesis testing to stop its growth. They define a hypothesis that the tree with a fixed size for the leaves has a certain perfomance (information gain) and an alternative hypothesis that if the tree grows it gets a higher perfomance. If the hypothsis test concludes that it was due chance that growing the tree gives higher performance, it stops. Otherwise, if the alternative hypothesis is accepeted, then it continues to grow.\n",
    "   \n",
    "   \n",
    "   2. let it grow, and prune it after\n",
    "    \n",
    "    <p> Pruning means to cut off leaves and branches, replacing them with leaves.\n",
    "        One general idea is to estimate whether replacing a set of leaves or a branch with a leaf would reduce accuracy. If not, then go ahead and prune. \n",
    "\n",
    "        \n",
    "### A General Method for Avoiding Overfitting\n",
    "        \n",
    "The nested houldout testing and nested cross validation techniques are introduced.\n",
    "        \n",
    "$$\\color{blue}{\\fbox{DATASET}}$$\n",
    "        \n",
    "$$\\swarrow \\searrow$$\n",
    "        \n",
    "$$\\color{green}{\\fbox{TRAIN SET}}\\hspace{12pt} \\color{red}{\\fbox{TEST SET}}$$\n",
    "        \n",
    "$$\\hspace{-2.5cm}\\swarrow \\searrow$$\n",
    "        \n",
    "$$\\hspace{-2.5cm}\\color{purple}{\\fbox{TRAIN SUBSET}} \\hspace{12pt} \\color{purple}{\\fbox{TEST SUBSET}}$$\n",
    "        \n",
    "In the nested cross-validation, for each fold we first run this experiment to find the parameter we want to tune, using another, smaller, cross-validation. If we used 5-fold cross-validation in both cases, we actually have built 30 total models (25 + 1*5) in the process.\n",
    "        \n",
    "### Avoiding Overfitting for Parameter Optimization\n",
    "        \n",
    "Instead of just optimizing the fit to the data, we optimize some combination of fit and simplicity. Models will be better if they fit the data better, but they also will be better if they are simpler. This general methodology is called regularization.\n",
    "        \n",
    "If we incorporate the L2-norm penalty into standard least-squares linear regression, we get the statistical procedure called ridge regression.  If instead we use the sum of the absolute values, known as the L1-norm, we get a procedure known as the lasso. L1-regularization ends up zeroing out many coefficients. Since these coefficients are the multiplicative weights on the features, L1-regularization effectively performs an automatic form of feature selection.\n",
    "\n",
    "How to  choose $\\lambda$ (the regularizer parameter)? Nested cross-validation on the training data. Then $\\lambda$ would be used to learn a regularized model on all the training data. This general approach to optimizing the parameter values of a data mining procedure is known as grid search.\n",
    "        \n",
    "### N.B.\n",
    "        \n",
    " Beware whenever someone does many tests and then picks the results that look good. Statistics books will warn against running multiple statistical hypothesis tests, and then looking at the ones that give “significant” results. These usually violate the assumptions behind the statistical tests, and the actual significance of the results is dubious. Even the procedures for avoiding overfitting themselves undertake multiple comparisons (e.g., choosing the best complexity for a model by comparing many complexities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
